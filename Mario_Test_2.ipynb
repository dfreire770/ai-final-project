{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNBnZSaVoNMBXNBqLGweq57",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dfreire770/dq-ddq-smb-agent/blob/main/Mario_Test_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a modified version of the DQN tutorial from here:\n",
        "\n",
        "https://colab.research.google.com/github/tensorflow/agents/blob/master/docs/tutorials/1_dqn_tutorial.ipynb#scrollTo=jbY4yrjTEyc9\n",
        "\n",
        "But, instead of Cartpole, it uses Super Mario Bros.\n",
        "\n",
        "The purpose of this is simply for an initial test of whether the DQN method can be used here in this fashion."
      ],
      "metadata": {
        "id": "NluZ-K8Yo1Vh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7aPnLKlT-RO"
      },
      "outputs": [],
      "source": [
        "\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y xvfb ffmpeg freeglut3-dev\n",
        "!pip install 'imageio==2.4.0'\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install tf-agents[reverb]\n",
        "!pip install pyglet\n",
        "!pip install tf-keras\n",
        "\n",
        "!pip install gym==0.26.2\n",
        "!pip install gym-notices==0.0.8\n",
        "!pip install gymnasium==0.29.1\n",
        "!pip install gym-super-mario-bros==7.4.0\n",
        "!pip install moviepy\n",
        "!pip install ffmpeg-python==0.2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a modified version of the DQN tutorial from here:\n",
        "\n",
        "https://colab.research.google.com/github/tensorflow/agents/blob/master/docs/tutorials/1_dqn_tutorial.ipynb#scrollTo=jbY4yrjTEyc9\n",
        "\n",
        "But, instead of Cartpole, it uses Super Mario Bros"
      ],
      "metadata": {
        "id": "DCmiBJCklvr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Keep using keras-2 (tf-keras) rather than keras-3 (keras).\n",
        "os.environ['TF_USE_LEGACY_KERAS'] = '1'"
      ],
      "metadata": {
        "id": "AM4udUwmUHHR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import pyvirtualdisplay\n",
        "import reverb\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.drivers import py_driver\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.eval import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.networks import sequential\n",
        "from tf_agents.policies import py_tf_eager_policy\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers import reverb_replay_buffer\n",
        "from tf_agents.replay_buffers import reverb_utils\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.utils import common\n",
        "\n",
        "\n",
        "import gym_super_mario_bros\n",
        "import gym.wrappers\n",
        "import gym\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import glob\n",
        "\n",
        "from IPython.display import HTML\n",
        "from IPython.display import Video\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, RIGHT_ONLY, COMPLEX_MOVEMENT\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "from base64 import b64encode\n",
        "\n",
        "from tf_agents.environments.tf_py_environment import TFPyEnvironment\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.specs import array_spec"
      ],
      "metadata": {
        "id": "V6zgpVUYULQp"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up a virtual display for rendering OpenAI gym environments.\n",
        "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()"
      ],
      "metadata": {
        "id": "oAJi21HHUOrY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.version.VERSION"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_GSNJHYaUReT",
        "outputId": "597e22f5-8602-4eeb-83e5-9960b3d391b9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.15.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_iterations = 20000 # @param {type:\"integer\"}\n",
        "\n",
        "initial_collect_steps = 100  # @param {type:\"integer\"}\n",
        "collect_steps_per_iteration =   1# @param {type:\"integer\"}\n",
        "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
        "\n",
        "batch_size = 64  # @param {type:\"integer\"}\n",
        "learning_rate = 1e-3  # @param {type:\"number\"}\n",
        "log_interval = 200  # @param {type:\"integer\"}\n",
        "\n",
        "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
        "eval_interval = 1000  # @param {type:\"integer\"}"
      ],
      "metadata": {
        "id": "YcuQf12oUVGI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym_super_mario_bros.make('SuperMarioBros-v0',\n",
        "                             apply_api_compatibility=True,\n",
        "                                render_mode=\"rgb_array\")\n",
        "\n",
        "#env_name = 'SuperMarioBros-v0'\n",
        "#env = suite_gym.load(env_name)"
      ],
      "metadata": {
        "id": "toyWeJlqUcNB"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset()\n",
        "PIL.Image.fromarray(env.render())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "iE4rDIIsUgcZ",
        "outputId": "f8dcf5a7-3d60-40cf-dc97-1b426191a120"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:272: UserWarning: \u001b[33mWARN: No render modes was declared in the environment (env.metadata['render_modes'] is None or not defined), you may have trouble when calling `.render()`.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=256x240>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GymWrapper(py_environment.PyEnvironment):\n",
        "    def __init__(self, gym_env):\n",
        "        super(GymWrapper, self).__init__()\n",
        "        self._gym_env = gym_env\n",
        "        self._action_spec = self._get_action_spec()\n",
        "        self._observation_spec = self._get_observation_spec()\n",
        "    def _get_action_spec(self):\n",
        "        action_space = self._gym_env.action_space\n",
        "        if isinstance(action_space, gym.spaces.Box):\n",
        "            return array_spec.BoundedArraySpec(\n",
        "                shape=action_space.shape,\n",
        "                dtype=action_space.dtype,\n",
        "                minimum=action_space.low,\n",
        "                maximum=action_space.high\n",
        "            )\n",
        "        elif isinstance(action_space, gym.spaces.Discrete):\n",
        "            return array_spec.BoundedArraySpec(\n",
        "                shape=(),\n",
        "                dtype=action_space.dtype,\n",
        "                minimum=0,\n",
        "                maximum=action_space.n-1\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported action space type: {type(action_space)}\")\n",
        "\n",
        "    def _get_observation_spec(self):\n",
        "        observation_space = self._gym_env.observation_space\n",
        "        return array_spec.ArraySpec(\n",
        "            shape=observation_space.shape,\n",
        "            dtype=observation_space.dtype\n",
        "        )\n",
        "\n",
        "    def action_spec(self):\n",
        "        return self._action_spec\n",
        "\n",
        "    def observation_spec(self):\n",
        "        return self._observation_spec\n",
        "\n",
        "    def _reset(self):\n",
        "        return ts.restart(self._gym_env.reset())\n",
        "\n",
        "    def _step(self, action):\n",
        "        obs, reward, done, info = self._gym_env.step(action)\n",
        "        if done:\n",
        "            return ts.termination(obs, reward)\n",
        "        else:\n",
        "            return ts.transition(obs, reward)"
      ],
      "metadata": {
        "id": "D0TUWX5citfs"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Observation Spec:')\n",
        "print(GymWrapper(env).time_step_spec().observation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJQXsr7pWYj0",
        "outputId": "3bd1ad40-fc0c-43d7-ed6c-62316a10b4f4"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation Spec:\n",
            "ArraySpec(shape=(240, 256, 3), dtype=dtype('uint8'), name=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Reward Spec:')\n",
        "print(GymWrapper(env).time_step_spec().reward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B40wdXtNW0sJ",
        "outputId": "72fd8c34-8d15-4a55-ca22-158f5acc8f55"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reward Spec:\n",
            "ArraySpec(shape=(), dtype=dtype('float32'), name='reward')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Action Spec:')\n",
        "print(GymWrapper(env).action_spec())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSftj9LNW4zI",
        "outputId": "64559f79-4958-4d76-ed39-e117f86378e5"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action Spec:\n",
            "BoundedArraySpec(shape=(), dtype=dtype('int64'), name=None, minimum=0, maximum=255)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "observation, info = env.reset()\n",
        "print('Time step:')\n",
        "print(observation, info)\n",
        "\n",
        "'''\n",
        "time_step = env.reset()\n",
        "print('Time step:')\n",
        "print(time_step)\n",
        "'''\n",
        "\n",
        "action = np.array(1, dtype=np.int32) # original comes with the other program but only returns 4 instead of 5\n",
        "#action = env.action_space.sample()\n",
        "\n",
        "\n",
        "observation, reward, terminated, truncated, info =  env.step(action)\n",
        "print('Next time step:')\n",
        "print(observation, reward, terminated, truncated, info)\n",
        "\n",
        "'''\n",
        "step_type, reward, discount, observation = env.step(action)\n",
        "print('Next time step:')\n",
        "print(step_type, reward, discount, observation )\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6tTFjS3LW8ur",
        "outputId": "a4b02369-a9d8-4c34-a814-28be22033865"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time step:\n",
            "[[[104 136 252]\n",
            "  [104 136 252]\n",
            "  [104 136 252]\n",
            "  ...\n",
            "  [104 136 252]\n",
            "  [104 136 252]\n",
            "  [104 136 252]]\n",
            "\n",
            " [[104 136 252]\n",
            "  [104 136 252]\n",
            "  [104 136 252]\n",
            "  ...\n",
            "  [104 136 252]\n",
            "  [104 136 252]\n",
            "  [104 136 252]]\n",
            "\n",
            " [[104 136 252]\n",
            "  [104 136 252]\n",
            "  [104 136 252]\n",
            "  ...\n",
            "  [104 136 252]\n",
            "  [104 136 252]\n",
            "  [104 136 252]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[240 208 176]\n",
            "  [228  92  16]\n",
            "  [228  92  16]\n",
            "  ...\n",
            "  [228  92  16]\n",
            "  [228  92  16]\n",
            "  [  0   0   0]]\n",
            "\n",
            " [[240 208 176]\n",
            "  [228  92  16]\n",
            "  [228  92  16]\n",
            "  ...\n",
            "  [228  92  16]\n",
            "  [  0   0   0]\n",
            "  [  0   0   0]]\n",
            "\n",
            " [[228  92  16]\n",
            "  [  0   0   0]\n",
            "  [  0   0   0]\n",
            "  ...\n",
            "  [  0   0   0]\n",
            "  [  0   0   0]\n",
            "  [228  92  16]]] {}\n",
            "Next time step:\n",
            "[[[104 136 252]\n",
            "  [104 136 252]\n",
            "  [104 136 252]\n",
            "  ...\n",
            "  [104 136 252]\n",
            "  [104 136 252]\n",
            "  [104 136 252]]\n",
            "\n",
            " [[104 136 252]\n",
            "  [104 136 252]\n",
            "  [104 136 252]\n",
            "  ...\n",
            "  [104 136 252]\n",
            "  [104 136 252]\n",
            "  [104 136 252]]\n",
            "\n",
            " [[104 136 252]\n",
            "  [104 136 252]\n",
            "  [104 136 252]\n",
            "  ...\n",
            "  [104 136 252]\n",
            "  [104 136 252]\n",
            "  [104 136 252]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[240 208 176]\n",
            "  [228  92  16]\n",
            "  [228  92  16]\n",
            "  ...\n",
            "  [228  92  16]\n",
            "  [228  92  16]\n",
            "  [  0   0   0]]\n",
            "\n",
            " [[240 208 176]\n",
            "  [228  92  16]\n",
            "  [228  92  16]\n",
            "  ...\n",
            "  [228  92  16]\n",
            "  [  0   0   0]\n",
            "  [  0   0   0]]\n",
            "\n",
            " [[228  92  16]\n",
            "  [  0   0   0]\n",
            "  [  0   0   0]\n",
            "  ...\n",
            "  [  0   0   0]\n",
            "  [  0   0   0]\n",
            "  [228  92  16]]] 0.0 False False {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'y_pos': 79}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nstep_type, reward, discount, observation = env.step(action)\\nprint('Next time step:')\\nprint(step_type, reward, discount, observation )\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_py_env = GymWrapper(env)\n",
        "eval_py_env = GymWrapper(env)\n",
        "# using GymWrapper function above in order to be able to use tensorflow next"
      ],
      "metadata": {
        "id": "aaFyaQicgxRy"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
        "# converting into tensorflow enviornment"
      ],
      "metadata": {
        "id": "Kl1E_vC5g_7R"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fc_layer_params = (100, 50)\n",
        "action_tensor_spec = tensor_spec.from_spec(GymWrapper(env).action_spec())\n",
        "# need GymWrapper(env) rather than just env\n",
        "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
        "\n",
        "# Define a helper function to create Dense layers configured with the right\n",
        "# activation and kernel initializer.\n",
        "def dense_layer(num_units):\n",
        "  return tf.keras.layers.Dense(\n",
        "      num_units,\n",
        "      activation=tf.keras.activations.relu,\n",
        "      kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
        "          scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
        "\n",
        "# QNetwork consists of a sequence of Dense layers followed by a dense layer\n",
        "# with `num_actions` units to generate one q_value per available action as\n",
        "# its output.\n",
        "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
        "q_values_layer = tf.keras.layers.Dense(\n",
        "    num_actions,\n",
        "    activation=None,\n",
        "    kernel_initializer=tf.keras.initializers.RandomUniform(\n",
        "        minval=-0.03, maxval=0.03),\n",
        "    bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
        "q_net = sequential.Sequential(dense_layers + [q_values_layer])"
      ],
      "metadata": {
        "id": "UTktYMNSj7YC"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO** - Fix following error -\n",
        "\n",
        "'ValueError: Expected q_network to emit a floating point tensor with inner dims (256,); but saw network output spec: TensorSpec(shape=(240, 256, 256), dtype=tf.float32, name=None)\n",
        "  In call to configurable 'DqnAgent' (<class 'tf_agents.agents.dqn.dqn_agent.DqnAgent'>)'"
      ],
      "metadata": {
        "id": "cC24HmnbpQph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "train_step_counter = tf.Variable(0)\n",
        "\n",
        "agent = dqn_agent.DqnAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    train_step_counter=train_step_counter)\n",
        "\n",
        "agent.initialize()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kX5WnzablAWe",
        "outputId": "a48e2f95-202f-4dc9-d7ad-0bcea231736a"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Expected q_network to emit a floating point tensor with inner dims (256,); but saw network output spec: TensorSpec(shape=(240, 256, 256), dtype=tf.float32, name=None)\n  In call to configurable 'DqnAgent' (<class 'tf_agents.agents.dqn.dqn_agent.DqnAgent'>)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-50adde309f92>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_step_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m agent = dqn_agent.DqnAgent(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtrain_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_step_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtrain_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gin/config.py\u001b[0m in \u001b[0;36mgin_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m       \u001b[0mscope_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" in scope '{}'\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscope_str\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mscope_str\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m       \u001b[0merr_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merr_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn_or_cls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m       \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment_exception_message_and_reraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mgin_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gin/utils.py\u001b[0m in \u001b[0;36maugment_exception_message_and_reraise\u001b[0;34m(exception, message)\u001b[0m\n\u001b[1;32m     39\u001b[0m   \u001b[0mproxy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExceptionProxy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0mExceptionProxy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mproxy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gin/config.py\u001b[0m in \u001b[0;36mgin_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1582\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1583\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m       \u001b[0merr_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_agents/agents/dqn/dqn_agent.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, time_step_spec, action_spec, q_network, optimizer, observation_and_action_constraint_splitter, epsilon_greedy, n_step_update, boltzmann_temperature, emit_log_probability, target_q_network, target_update_tau, target_update_period, td_errors_loss_fn, gamma, reward_scale_factor, gradient_clipping, debug_summaries, summarize_grads_and_vars, train_step_counter, training_data_spec, name)\u001b[0m\n\u001b[1;32m    241\u001b[0m     )\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_network_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_q_network\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'q_network'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_network_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_target_q_network\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'target_q_network'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_agents/agents/dqn/dqn_agent.py\u001b[0m in \u001b[0;36m_check_network_output\u001b[0;34m(self, net, label)\u001b[0m\n\u001b[1;32m    329\u001b[0m       \u001b[0mlabel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mto\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcase\u001b[0m \u001b[0mof\u001b[0m \u001b[0ma\u001b[0m \u001b[0mmismatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \"\"\"\n\u001b[0;32m--> 331\u001b[0;31m     network_utils.check_single_floating_network_output(\n\u001b[0m\u001b[1;32m    332\u001b[0m         \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0mexpected_output_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_actions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_agents/networks/utils.py\u001b[0m in \u001b[0;36mcheck_single_floating_network_output\u001b[0;34m(output_spec, expected_output_shape, label)\u001b[0m\n\u001b[1;32m     39\u001b[0m       \u001b[0;32mand\u001b[0m \u001b[0moutput_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m   ):\n\u001b[0;32m---> 41\u001b[0;31m     raise ValueError(\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;34m'Expected {} to emit a floating point tensor with inner dims '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         '{}; but saw network output spec: {}'.format(\n",
            "\u001b[0;31mValueError\u001b[0m: Expected q_network to emit a floating point tensor with inner dims (256,); but saw network output spec: TensorSpec(shape=(240, 256, 256), dtype=tf.float32, name=None)\n  In call to configurable 'DqnAgent' (<class 'tf_agents.agents.dqn.dqn_agent.DqnAgent'>)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ooDxo0eeluPy"
      }
    }
  ]
}