{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zR5l7a-sPne"
      },
      "outputs": [],
      "source": [
        "!pip install gym==0.26.2\n",
        "!pip install gym-notices==0.0.8\n",
        "!pip install gymnasium==0.29.1\n",
        "!pip install gym-super-mario-bros==7.4.0\n",
        "!pip install moviepy\n",
        "#!pip install ffmpeg-python==0.2.0\n",
        "\n",
        "#!pip install gym-super-mario-bros\n",
        "!pip --disable-pip-version-check install -q nes_py\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y xvfb ffmpeg freeglut3-dev\n",
        "!pip --disable-pip-version-check install -q pyvirtualdisplay\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: numpy in /Users/diegofreire/anaconda3/envs/super-mario-rl/lib/python3.12/site-packages (from tensorboardX) (1.26.4)\n",
            "Requirement already satisfied: packaging in /Users/diegofreire/anaconda3/envs/super-mario-rl/lib/python3.12/site-packages (from tensorboardX) (24.0)\n",
            "Requirement already satisfied: protobuf>=3.20 in /Users/diegofreire/anaconda3/envs/super-mario-rl/lib/python3.12/site-packages (from tensorboardX) (4.25.3)\n",
            "Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorboardX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zaPSChcvtav4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import transforms as T\n",
        "\n",
        "import time, datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from collections import deque\n",
        "import random, datetime, os, copy\n",
        "\n",
        "import gym\n",
        "from gym.spaces import Box\n",
        "from gym.wrappers import FrameStack\n",
        "\n",
        "# change directory\n",
        "import os\n",
        "#os.chdir('/kaggle/input/gym-super-mario-bros'). <-- MAY NEED CHANGE HERE\n",
        "\n",
        "# install controller\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
        "from gym.wrappers import FrameStack, GrayScaleObservation, TransformObservation\n",
        "\n",
        "#from pyvirtualdisplay import Display\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "from IPython.display import HTML\n",
        "\n",
        "#from gym.wrappers import Monitor -- DEPRECATED\n",
        "#from gym.wrappers.record_video import RecordVideo\n",
        "from glob import glob\n",
        "\n",
        "import base64\n",
        "import io\n",
        "\n",
        "import cv2\n",
        "\n",
        "import time\n",
        "from time import sleep\n",
        "import datetime\n",
        "\n",
        "#from gym_recorder import Recorder\n",
        "\n",
        "#display = Display(visible=0, size=(600, 300))\n",
        "#display.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7AyLSER7txfs"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import torch\n",
        "import random, datetime, numpy as np\n",
        "from skimage import transform\n",
        "\n",
        "from gym.spaces import Box\n",
        "\n",
        "class ResizeObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env, shape):\n",
        "        super().__init__(env)\n",
        "        if isinstance(shape, int):\n",
        "            self.shape = (shape, shape)\n",
        "        else:\n",
        "            self.shape = tuple(shape)\n",
        "\n",
        "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
        "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        resize_obs = transform.resize(observation, self.shape)\n",
        "        # cast float back to uint8\n",
        "        resize_obs *= 255\n",
        "        resize_obs = resize_obs.astype(np.uint8)\n",
        "        return resize_obs\n",
        "\n",
        "\n",
        "class SkipFrame(gym.Wrapper):\n",
        "    def __init__(self, env, skip):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        super().__init__(env)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Repeat action, and sum reward\"\"\"\n",
        "        total_reward = 0.0\n",
        "        done = False\n",
        "        for i in range(self._skip):\n",
        "            # Accumulate reward and repeat the same action\n",
        "            obs, reward, done, truncated, info = self.env.step(action)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        return obs, total_reward, done, truncated, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BkikVdrpuFjN"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MarioNet(nn.Module):\n",
        "    \"\"\"\n",
        "        input -> (conv2d + relu) x 3 -> flatten -> (dense + relu) x 2 -> output\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        c, w, h = input_dim\n",
        "\n",
        "        if h != 84: raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
        "        if w != 84: raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
        "\n",
        "        self.online = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(3136, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, output_dim)\n",
        "        )\n",
        "\n",
        "        self.target = copy.deepcopy(self.online)\n",
        "\n",
        "        # freeze Q-target parameters\n",
        "        for p in self.target.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def forward(self, input, model):\n",
        "        if model == \"online\":\n",
        "            return self.online(input)\n",
        "        elif model == \"target\":\n",
        "            return self.target(input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QnN1KDQWCAjB"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Mario:\n",
        "    def __init__(self, state_dim, action_dim, save_dir, checkpoint=None):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.memory = deque(maxlen=100000)\n",
        "        self.batch_size = 32\n",
        "\n",
        "        self.exploration_rate = 1\n",
        "        self.exploration_rate_decay = 0.99999975\n",
        "        self.exploration_rate_min = 0.1\n",
        "        self.gamma = 0.9\n",
        "\n",
        "        self.curr_step = 0\n",
        "        self.burnin = 1e5  # min. experiences before training\n",
        "        self.learn_every = 3   # no. of experiences between updates to Q_online\n",
        "        self.sync_every = 1e4   # no. of experiences between Q_target & Q_online sync\n",
        "\n",
        "        self.save_every = 5e5   # no. of experiences between saving Mario Net\n",
        "        self.save_dir = save_dir\n",
        "\n",
        "        self.use_cuda = torch.cuda.is_available()\n",
        "\n",
        "        # Mario's DNN to predict the most optimal action - we implement this in the Learn section\n",
        "        self.net = MarioNet(self.state_dim, self.action_dim).float()\n",
        "        if self.use_cuda:\n",
        "            self.net = self.net.to(device='cuda')\n",
        "        if checkpoint:\n",
        "            self.load(checkpoint)\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
        "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
        "\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"\n",
        "        Given a state, choose an epsilon-greedy action and update value of step.\n",
        "\n",
        "        Inputs:\n",
        "        state(LazyFrame): A single observation of the current state, dimension is (state_dim)\n",
        "        Outputs:\n",
        "        action_idx (int): An integer representing which action Mario will perform\n",
        "        \"\"\"\n",
        "        # EXPLORE\n",
        "        if np.random.rand() < self.exploration_rate:\n",
        "            action_idx = np.random.randint(self.action_dim)\n",
        "\n",
        "        # EXPLOIT\n",
        "        else:\n",
        "            state = torch.FloatTensor(state).cuda() if self.use_cuda else torch.FloatTensor(state)\n",
        "            state = state.unsqueeze(0)\n",
        "            action_values = self.net(state, model='online')\n",
        "            action_idx = torch.argmax(action_values, axis=1).item()\n",
        "\n",
        "        # decrease exploration_rate\n",
        "        self.exploration_rate *= self.exploration_rate_decay\n",
        "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
        "\n",
        "        # increment step\n",
        "        self.curr_step += 1\n",
        "        return action_idx\n",
        "\n",
        "    def cache(self, state, next_state, action, reward, done):\n",
        "        \"\"\"\n",
        "        Store the experience to self.memory (replay buffer)\n",
        "\n",
        "        Inputs:\n",
        "        state (LazyFrame),\n",
        "        next_state (LazyFrame),\n",
        "        action (int),\n",
        "        reward (float),\n",
        "        done(bool))\n",
        "        \"\"\"\n",
        "        state = torch.FloatTensor(state).cuda() if self.use_cuda else torch.FloatTensor(state)\n",
        "        next_state = torch.FloatTensor(next_state).cuda() if self.use_cuda else torch.FloatTensor(next_state)\n",
        "        action = torch.LongTensor([action]).cuda() if self.use_cuda else torch.LongTensor([action])\n",
        "        reward = torch.DoubleTensor([reward]).cuda() if self.use_cuda else torch.DoubleTensor([reward])\n",
        "        done = torch.BoolTensor([done]).cuda() if self.use_cuda else torch.BoolTensor([done])\n",
        "\n",
        "        self.memory.append( (state, next_state, action, reward, done,) )\n",
        "\n",
        "\n",
        "    def recall(self):\n",
        "        \"\"\"\n",
        "        Retrieve a batch of experiences from memory\n",
        "        \"\"\"\n",
        "        batch = random.sample(self.memory, self.batch_size)\n",
        "        state, next_state, action, reward, done = map(torch.stack, zip(*batch))\n",
        "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()\n",
        "\n",
        "\n",
        "    def td_estimate(self, state, action):\n",
        "        current_Q = self.net(state, model='online')[np.arange(0, self.batch_size), action] # Q_online(s,a)\n",
        "        return current_Q\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def td_target(self, reward, next_state, done):\n",
        "        next_state_Q = self.net(next_state, model='online')\n",
        "        best_action = torch.argmax(next_state_Q, axis=1)\n",
        "        next_Q = self.net(next_state, model='target')[np.arange(0, self.batch_size), best_action]\n",
        "        return (reward + (1 - done.float()) * self.gamma * next_Q).float()\n",
        "\n",
        "\n",
        "    def update_Q_online(self, td_estimate, td_target) :\n",
        "        loss = self.loss_fn(td_estimate, td_target)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss.item()\n",
        "\n",
        "\n",
        "    def sync_Q_target(self):\n",
        "        self.net.target.load_state_dict(self.net.online.state_dict())\n",
        "\n",
        "\n",
        "    def learn(self):\n",
        "        if self.curr_step % self.sync_every == 0:\n",
        "            self.sync_Q_target()\n",
        "\n",
        "        if self.curr_step % self.save_every == 0:\n",
        "            self.save()\n",
        "\n",
        "        if self.curr_step < self.burnin:\n",
        "            return None, None\n",
        "\n",
        "        if self.curr_step % self.learn_every != 0:\n",
        "            return None, None\n",
        "\n",
        "        # Sample from memory\n",
        "        state, next_state, action, reward, done = self.recall()\n",
        "\n",
        "        # Get TD Estimate\n",
        "        td_est = self.td_estimate(state, action)\n",
        "\n",
        "        # Get TD Target\n",
        "        td_tgt = self.td_target(reward, next_state, done)\n",
        "\n",
        "        # Backpropagate loss through Q_online\n",
        "        loss = self.update_Q_online(td_est, td_tgt)\n",
        "\n",
        "        return (td_est.mean().item(), loss)\n",
        "\n",
        "\n",
        "    def save(self):\n",
        "        save_path = self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.chkpt\"\n",
        "        torch.save(\n",
        "            dict(\n",
        "                model=self.net.state_dict(),\n",
        "                exploration_rate=self.exploration_rate\n",
        "            ),\n",
        "            save_path\n",
        "        )\n",
        "        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")\n",
        "\n",
        "\n",
        "    def load(self, load_path):\n",
        "        if not load_path.exists():\n",
        "            raise ValueError(f\"{load_path} does not exist\")\n",
        "\n",
        "        ckp = torch.load(load_path, map_location=('cuda' if self.use_cuda else 'cpu'))\n",
        "        exploration_rate = ckp.get('exploration_rate')\n",
        "        state_dict = ckp.get('model')\n",
        "\n",
        "        print(f\"Loading model at {load_path} with exploration rate {exploration_rate}\")\n",
        "        self.net.load_state_dict(state_dict)\n",
        "        self.exploration_rate = exploration_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xMG3hp_LuOsi"
      },
      "outputs": [],
      "source": [
        "class MetricLogger():\n",
        "    def __init__(self, save_dir):\n",
        "        self.save_log = save_dir / \"log\"\n",
        "        with open(self.save_log, \"w\") as f:\n",
        "            f.write(\n",
        "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
        "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
        "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
        "            )\n",
        "\n",
        "        self.writer = SummaryWriter(str(save_dir))  # Initialize SummaryWriter\n",
        "\n",
        "        # History metrics\n",
        "        self.ep_rewards = []\n",
        "        self.ep_lengths = []\n",
        "        self.ep_avg_losses = []\n",
        "        self.ep_avg_qs = []\n",
        "\n",
        "        # Current episode metric\n",
        "        self.init_episode()\n",
        "\n",
        "        # Timing\n",
        "        self.record_time = time.time()\n",
        "\n",
        "    def log_step(self, reward, loss, q):\n",
        "        self.curr_ep_reward += reward\n",
        "        self.curr_ep_length += 1\n",
        "        if loss:\n",
        "            self.curr_ep_loss += loss\n",
        "            self.curr_ep_q += q\n",
        "            self.curr_ep_loss_length += 1\n",
        "\n",
        "    def log_episode(self):\n",
        "        \"Mark end of episode\"\n",
        "        self.ep_rewards.append(self.curr_ep_reward)\n",
        "        self.ep_lengths.append(self.curr_ep_length)\n",
        "        if self.curr_ep_loss_length == 0:\n",
        "            ep_avg_loss = 0\n",
        "            ep_avg_q = 0\n",
        "        else:\n",
        "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
        "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
        "        self.ep_avg_losses.append(ep_avg_loss)\n",
        "        self.ep_avg_qs.append(ep_avg_q)\n",
        "\n",
        "        self.init_episode()\n",
        "\n",
        "    def init_episode(self):\n",
        "        self.curr_ep_reward = 0.0\n",
        "        self.curr_ep_length = 0\n",
        "        self.curr_ep_loss = 0.0\n",
        "        self.curr_ep_q = 0.0\n",
        "        self.curr_ep_loss_length = 0\n",
        "\n",
        "    def record(self, episode, epsilon, step):\n",
        "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
        "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
        "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
        "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
        "\n",
        "        last_record_time = self.record_time\n",
        "        self.record_time = time.time()\n",
        "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
        "\n",
        "        print(\n",
        "            f\"Episode {episode} - \"\n",
        "            f\"Step {step} - \"\n",
        "            f\"Epsilon {epsilon} - \"\n",
        "            f\"Mean Reward {mean_ep_reward} - \"\n",
        "            f\"Mean Length {mean_ep_length} - \"\n",
        "            f\"Mean Loss {mean_ep_loss} - \"\n",
        "            f\"Mean Q Value {mean_ep_q} - \"\n",
        "            f\"Time Delta {time_since_last_record} - \"\n",
        "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
        "        )\n",
        "\n",
        "        with open(self.save_log, \"a\") as f:\n",
        "            f.write(\n",
        "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
        "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
        "                f\"{time_since_last_record:15.3f}\"\n",
        "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
        "            )\n",
        "\n",
        "        # Log metrics to Tensorboard\n",
        "        self.writer.add_scalar('Mean Reward', mean_ep_reward, episode)\n",
        "        self.writer.add_scalar('Mean Length', mean_ep_length, episode)\n",
        "        self.writer.add_scalar('Mean Loss', mean_ep_loss, episode)\n",
        "        self.writer.add_scalar('Mean Q Value', mean_ep_q, episode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model at trained_mario.chkpt with exploration rate 0.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/diegofreire/anaconda3/envs/super-mario-rl/lib/python3.12/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-1-1-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
            "  logger.warn(\n",
            "/Users/diegofreire/anaconda3/envs/super-mario-rl/lib/python3.12/site-packages/gym/envs/registration.py:627: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ],
      "source": [
        "# Initialize Super Mario environment\n",
        "#env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
        "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0', apply_api_compatibility=True, render_mode=\"rgb_array\")\n",
        "\n",
        "# Limit the action-space to\n",
        "#   0. walk right\n",
        "#   1. jump right\n",
        "env = JoypadSpace(\n",
        "    env,\n",
        "    [['right'],\n",
        "    ['right', 'A']]\n",
        ")\n",
        "\n",
        "# Apply Wrappers to environment\n",
        "env = SkipFrame(env, skip=4)\n",
        "env = GrayScaleObservation(env, keep_dim=False)\n",
        "env = ResizeObservation(env, shape=84)\n",
        "env = TransformObservation(env, f=lambda x: x / 255.)\n",
        "env = FrameStack(env, num_stack=4)\n",
        "\n",
        "save_dir = Path('checkpoints') / datetime.datetime.now().strftime('%Y-%m-%dT%H-%M-%S')\n",
        "save_dir.mkdir(parents=True)\n",
        "\n",
        "#checkpoint = None # Path('checkpoints/2020-10-21T18-25-27/mario.chkpt')\n",
        "checkpoint = Path('trained_mario.chkpt')\n",
        "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir, checkpoint=checkpoint)\n",
        "\n",
        "logger = MetricLogger(save_dir)\n",
        "\n",
        "#episodes = 40000\n",
        "episodes = 100\n",
        "record_interval = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "id": "yjzuPvNkuVbk",
        "outputId": "961ead7b-f7b8-4008-892f-b93e2962f76f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 0 - Step 168 - Epsilon 0.1 - Mean Reward 810.0 - Mean Length 168.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 5.473 - Time 2024-05-12T20:58:16\n",
            "Episode 10 - Step 3157 - Epsilon 0.1 - Mean Reward 1090.636 - Mean Length 287.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 32.895 - Time 2024-05-12T20:58:49\n",
            "Episode 20 - Step 5347 - Epsilon 0.1 - Mean Reward 1081.619 - Mean Length 254.619 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 23.952 - Time 2024-05-12T20:59:13\n",
            "Episode 30 - Step 7718 - Epsilon 0.1 - Mean Reward 1091.097 - Mean Length 248.968 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 26.054 - Time 2024-05-12T20:59:39\n",
            "Episode 40 - Step 9726 - Epsilon 0.1 - Mean Reward 1104.732 - Mean Length 237.22 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 22.01 - Time 2024-05-12T21:00:01\n",
            "Episode 50 - Step 11750 - Epsilon 0.1 - Mean Reward 1076.804 - Mean Length 230.392 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 21.867 - Time 2024-05-12T21:00:23\n",
            "Episode 60 - Step 14102 - Epsilon 0.1 - Mean Reward 1097.574 - Mean Length 231.18 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 25.417 - Time 2024-05-12T21:00:49\n",
            "Episode 70 - Step 16220 - Epsilon 0.1 - Mean Reward 1087.268 - Mean Length 228.451 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 23.043 - Time 2024-05-12T21:01:12\n",
            "Episode 80 - Step 18173 - Epsilon 0.1 - Mean Reward 1093.309 - Mean Length 224.358 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 21.356 - Time 2024-05-12T21:01:33\n",
            "Episode 90 - Step 22201 - Epsilon 0.1 - Mean Reward 1108.407 - Mean Length 243.967 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 43.661 - Time 2024-05-12T21:02:17\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Training loop\n",
        "for e in range(episodes):\n",
        "    video = None\n",
        "    \n",
        "    if e % record_interval == 0:  # Check if it's time to create a recording\n",
        "        video = cv2.VideoWriter(f'mario-rl-{e}.mp4', cv2.VideoWriter_fourcc(*'mp4v'), 30, (256,240))\n",
        "        \n",
        "    state, info = env.reset()\n",
        "    # Play the game!\n",
        "    while True:\n",
        "        if video is not None:\n",
        "            video.write(cv2.cvtColor(env.render(), cv2.COLOR_RGB2BGR))\n",
        "        \n",
        "        # Run agent on the state\n",
        "        action = mario.act(state)\n",
        "\n",
        "        # Agent performs action\n",
        "        next_state, reward, done, truncated, info = env.step(action)\n",
        "                \n",
        "        # Remember\n",
        "        mario.cache(state, next_state, action, reward, done)\n",
        "\n",
        "        # Learn\n",
        "        q, loss = mario.learn()\n",
        "\n",
        "        # Logging\n",
        "        logger.log_step(reward, loss, q)\n",
        "\n",
        "        # Update state\n",
        "        state = next_state\n",
        "\n",
        "        # Check if end of game\n",
        "        if done or info['flag_get']:\n",
        "            break\n",
        "\n",
        "    logger.log_episode()\n",
        "    if e % record_interval == 0:\n",
        "        if video is not None:\n",
        "            video.release()\n",
        "            \n",
        "        # Log stuff \n",
        "        logger.record(\n",
        "            episode=e,\n",
        "            epsilon=mario.exploration_rate,\n",
        "            step=mario.curr_step\n",
        "        )\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# replay\n",
        "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0', apply_api_compatibility=True, render_mode=\"human\")\n",
        "\n",
        "checkpoint = Path('./trained_mario.chkpt')\n",
        "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir, checkpoint=checkpoint)\n",
        "mario.exploration_rate = mario.exploration_rate_min\n",
        "\n",
        "logger = MetricLogger(save_dir)\n",
        "\n",
        "episodes = 1000\n",
        "\n",
        "for e in range(episodes):\n",
        "\n",
        "    state, info = env.reset()\n",
        "\n",
        "    while True:\n",
        "\n",
        "        env.render()\n",
        "\n",
        "        action = mario.act(state)\n",
        "        #obs, reward, terminated, truncated, info\n",
        "        next_state, reward, done, truncated, info = env.step(action)\n",
        "\n",
        "        mario.cache(state, next_state, action, reward, done)\n",
        "\n",
        "        logger.log_step(reward, None, None)\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        if done or info['flag_get']:\n",
        "            break\n",
        "\n",
        "    logger.log_episode()\n",
        "\n",
        "    if e % 20 == 0:\n",
        "        logger.record(\n",
        "            episode=e,\n",
        "            epsilon=mario.exploration_rate,\n",
        "            step=mario.curr_step\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
