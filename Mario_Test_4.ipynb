{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dfreire770/dq-ddq-smb-agent/blob/main/Mario_Test_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajDc9Xv487hR"
      },
      "source": [
        "CODE COMES FROM HERE\n",
        "\n",
        "https://www.kaggle.com/code/alincijov/super-mario-bros-double-deep-q-networks/notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAfMUS249Qth",
        "outputId": "d1a06495-b968-4ab7-8dc5-30ad567b4385"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym-super-mario-bros in /usr/local/lib/python3.10/dist-packages (7.4.0)\n",
            "Requirement already satisfied: nes-py>=8.1.4 in /usr/local/lib/python3.10/dist-packages (from gym-super-mario-bros) (8.2.1)\n",
            "Requirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.10/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros) (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros) (1.25.2)\n",
            "Requirement already satisfied: pyglet<=1.5.21,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros) (1.5.21)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.10/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros) (4.66.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros) (0.0.8)\n",
            "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "freeglut3-dev is already the newest version (2.8.1-6).\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "xvfb is already the newest version (2:21.1.4-2ubuntu1.7~22.04.10).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 51 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!pip install gym-super-mario-bros\n",
        "!pip --disable-pip-version-check install -q nes_py\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y xvfb ffmpeg freeglut3-dev\n",
        "!pip --disable-pip-version-check install -q pyvirtualdisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njCCRUgH8sW8",
        "outputId": "c88b4606-8911-4477-dc65-408cc93edf28"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7e06ff78a950>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import transforms as T\n",
        "\n",
        "import time, datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from collections import deque\n",
        "import random, datetime, os, copy\n",
        "\n",
        "import gym\n",
        "from gym.spaces import Box\n",
        "from gym.wrappers import FrameStack\n",
        "\n",
        "# change directory\n",
        "import os\n",
        "#os.chdir('/kaggle/input/gym-super-mario-bros'). <-- MAY NEED CHANGE HERE\n",
        "\n",
        "# install controller\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "from IPython.display import HTML\n",
        "\n",
        "#from gym.wrappers import Monitor -- DEPRECATED\n",
        "from gym.wrappers.record_video import RecordVideo\n",
        "from glob import glob\n",
        "\n",
        "import base64\n",
        "import io\n",
        "\n",
        "display = Display(visible=0, size=(600, 300))\n",
        "display.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1Yl-IvC9ntt",
        "outputId": "9ba22af9-cdec-4869-df82-fc7d3f2e32cf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(done, (bool, np.bool8)):\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:280: UserWarning: \u001b[33mWARN: No render modes was declared in the environment (env.metadata['render_modes'] is None or not defined), you may have trouble when calling `.render()`.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ],
      "source": [
        "from nes_py.wrappers import JoypadSpace\n",
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
        "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
        "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
        "\n",
        "done = True\n",
        "for step in range(5000):\n",
        "    if done:\n",
        "        state = env.reset()\n",
        "    state, reward, done, info = env.step(env.action_space.sample())\n",
        "    env.render(mode='rgb_array')\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAVLkfzv-7bE",
        "outputId": "f936133d-ab70-4dce-da62-146bf4419ed8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-1-1-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(240, 256, 3),\n",
            " 0.0,\n",
            " False,\n",
            " {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'y_pos': 79}\n"
          ]
        }
      ],
      "source": [
        "env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\")\n",
        "\n",
        "# Limit the action-space to\n",
        "#   0. walk right\n",
        "#   1. jump right\n",
        "env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
        "\n",
        "env.reset()\n",
        "next_state, reward, done, info = env.step(action=0)\n",
        "print(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKirapbs_C8C"
      },
      "outputs": [],
      "source": [
        "class SkipFrame(gym.Wrapper):\n",
        "    '''\n",
        "        Custom wrapper that inherits from gy.Wrapper and implements the step() function.\n",
        "        Use it to return only every skip nth frame\n",
        "    '''\n",
        "    def __init__(self, env, skip):\n",
        "        super().__init__(env)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        total_reward = 0.0\n",
        "        done = False\n",
        "        for i in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        return obs, total_reward, done, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tmiOHlN_MTz"
      },
      "outputs": [],
      "source": [
        "class ResizeObservation(gym.ObservationWrapper):\n",
        "    '''\n",
        "        Downsamples each observation into a square image.\n",
        "        New size: [1, 84, 84]\n",
        "    '''\n",
        "    def __init__(self, env, shape):\n",
        "        super().__init__(env)\n",
        "        if isinstance(shape, int): self.shape = (shape, shape)\n",
        "        else: self.shape = tuple(shape)\n",
        "\n",
        "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
        "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        transforms = T.Compose([T.Resize(self.shape), T.Normalize(0, 255)])\n",
        "        return transforms(observation).squeeze(0)\n",
        "\n",
        "class GrayScaleObservation(gym.ObservationWrapper):\n",
        "    '''\n",
        "        Common wrapper to transform an RGB image to grayscale; doing so reduces the size of the state representation without losing useful information.\n",
        "        Now the size of each state: [1, 240, 256]\n",
        "    '''\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        obs_shape = self.observation_space.shape[:2]\n",
        "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "        self.transform = T.Grayscale()\n",
        "\n",
        "    def permute_orientation(self, observation):\n",
        "        observation = np.transpose(observation, (2, 0, 1))\n",
        "        return torch.tensor(observation.copy(), dtype=torch.float)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        observation = self.permute_orientation(observation)\n",
        "        return self.transform(observation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73XW1nY5_RyU"
      },
      "outputs": [],
      "source": [
        "# preprocess environment\n",
        "env = SkipFrame(env, skip=4)\n",
        "env = GrayScaleObservation(env)\n",
        "env = ResizeObservation(env, shape=84)\n",
        "env = FrameStack(env, num_stack=4)\n",
        "\n",
        "# record every 25th episode\n",
        "#env = RecordVideo(env, './video', episode_trigger=lambda x: x % 25 == 0) <---- WHEN TRYING THIS, IT CRASHES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZVl1UHv_c5j"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MarioNet(nn.Module):\n",
        "    \"\"\"\n",
        "        input -> (conv2d + relu) x 3 -> flatten -> (dense + relu) x 2 -> output\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        c, w, h = input_dim\n",
        "\n",
        "        if h != 84: raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
        "        if w != 84: raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
        "\n",
        "        self.online = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(3136, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, output_dim)\n",
        "        )\n",
        "\n",
        "        self.target = copy.deepcopy(self.online)\n",
        "\n",
        "        # freeze Q-target parameters\n",
        "        for p in self.target.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def forward(self, input, model):\n",
        "        if model == \"online\":\n",
        "            return self.online(input)\n",
        "        elif model == \"target\":\n",
        "            return self.target(input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWlFhlz-_jgs"
      },
      "outputs": [],
      "source": [
        "class Mario:\n",
        "    '''\n",
        "        Mario randomly explores with a chance of self.exploration_rate.\n",
        "        When he chooses to exploit, he relies on MarioNet to provide the most optimal action.\n",
        "    '''\n",
        "    def __init__(self, state_dim, action_dim, use_cuda):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.use_cuda = use_cuda\n",
        "        self.memory = deque(maxlen=100000)\n",
        "        self.batch_size = 32\n",
        "\n",
        "        self.net = MarioNet(self.state_dim, self.action_dim).float()\n",
        "        if self.use_cuda:\n",
        "            self.net = self.net.to(device=\"cuda\")\n",
        "\n",
        "        self.exploration_rate = 1\n",
        "        self.exploration_rate_decay = 0.99999975\n",
        "        self.exploration_rate_min = 0.1\n",
        "        self.curr_step = 0\n",
        "\n",
        "        self.save_every = 5e5\n",
        "\n",
        "\n",
        "    def act(self, state):\n",
        "        '''\n",
        "            Given a state, choose an epsilon-greedy action and update value of step.\n",
        "        '''\n",
        "        # exploration\n",
        "        if np.random.rand() < self.exploration_rate:\n",
        "            action_idx = np.random.randint(self.action_dim)\n",
        "\n",
        "        # exploitation\n",
        "        else:\n",
        "            state = state.__array__()\n",
        "\n",
        "            if self.use_cuda: state = torch.tensor(state).cuda()\n",
        "            else: state = torch.tensor(state)\n",
        "\n",
        "            state = state.unsqueeze(0)\n",
        "            action_values = self.net(state, model=\"online\")\n",
        "            action_idx = torch.argmax(action_values, axis=1).item()\n",
        "\n",
        "        # decrease exploration_rate\n",
        "        self.exploration_rate *= self.exploration_rate_decay\n",
        "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
        "\n",
        "        # increment step\n",
        "        self.curr_step += 1\n",
        "        return action_idx\n",
        "\n",
        "\n",
        "    def cache(self, state, next_state, action, reward, done):\n",
        "        \"\"\"\n",
        "            Store the experience to self.memory (replay buffer).\n",
        "        \"\"\"\n",
        "        state = state.__array__()\n",
        "        next_state = next_state.__array__()\n",
        "\n",
        "        state = torch.tensor(state)\n",
        "        next_state = torch.tensor(next_state)\n",
        "        action = torch.tensor([action])\n",
        "        reward = torch.tensor([reward])\n",
        "        done = torch.tensor([done])\n",
        "\n",
        "        if self.use_cuda:\n",
        "            state = state.cuda()\n",
        "            next_state = next_state.cuda()\n",
        "            action = action.cuda()\n",
        "            reward = reward.cuda()\n",
        "            done = done.cuda()\n",
        "\n",
        "        self.memory.append((state, next_state, action, reward, done,))\n",
        "\n",
        "\n",
        "    def recall(self):\n",
        "        \"\"\"\n",
        "            Retrieve a batch of experiences from memory\n",
        "        \"\"\"\n",
        "        batch = random.sample(self.memory, self.batch_size)\n",
        "        state, next_state, action, reward, done = map(torch.stack, zip(*batch))\n",
        "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CarqFbzi_pde"
      },
      "outputs": [],
      "source": [
        "class Mario(Mario):\n",
        "    def __init__(self, state_dim, action_dim, use_cuda):\n",
        "        super().__init__(state_dim, action_dim, use_cuda)\n",
        "        self.gamma = 0.9\n",
        "        self.burnin = 1e4  # min. experiences before training\n",
        "        self.learn_every = 3  # no. of experiences between updates to Q_online\n",
        "        self.sync_every = 1e4  # no. of experiences between Q_target & Q_online sync\n",
        "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
        "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
        "\n",
        "\n",
        "    def td_estimate(self, state, action):\n",
        "        current_Q = self.net(state, model=\"online\")[\n",
        "            np.arange(0, self.batch_size), action\n",
        "        ]\n",
        "        return current_Q\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def td_target(self, reward, next_state, done):\n",
        "        next_state_Q = self.net(next_state, model=\"online\")\n",
        "        best_action = torch.argmax(next_state_Q, axis=1)\n",
        "        next_Q = self.net(next_state, model=\"target\")[\n",
        "            np.arange(0, self.batch_size), best_action\n",
        "        ]\n",
        "        return (reward + (1 - done.float()) * self.gamma * next_Q).float()\n",
        "\n",
        "\n",
        "    def update_Q_online(self, td_estimate, td_target):\n",
        "        loss = self.loss_fn(td_estimate, td_target)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss.item()\n",
        "\n",
        "\n",
        "    def sync_Q_target(self):\n",
        "        self.net.target.load_state_dict(self.net.online.state_dict())\n",
        "\n",
        "\n",
        "    def learn(self):\n",
        "        if self.curr_step % self.sync_every == 0: self.sync_Q_target()\n",
        "        if self.curr_step % self.save_every == 0: self.save()\n",
        "        if self.curr_step < self.burnin: return None, None\n",
        "        if self.curr_step % self.learn_every != 0: return None, None\n",
        "\n",
        "        # Sample from memory\n",
        "        state, next_state, action, reward, done = self.recall()\n",
        "\n",
        "        # Get TD Estimate\n",
        "        td_est = self.td_estimate(state, action)\n",
        "\n",
        "        # Get TD Target\n",
        "        td_tgt = self.td_target(reward, next_state, done)\n",
        "\n",
        "        # Backpropagate loss through Q_online\n",
        "        loss = self.update_Q_online(td_est, td_tgt)\n",
        "\n",
        "        return (td_est.mean().item(), loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzeDcDOq_uoL"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MetricLogger:\n",
        "    def __init__(self):\n",
        "        # history metrics\n",
        "        self.ep_rewards = []\n",
        "        self.ep_lengths = []\n",
        "        self.ep_avg_losses = []\n",
        "        self.ep_avg_qs = []\n",
        "\n",
        "        # moving averages, added for every call to record()\n",
        "        self.moving_avg_ep_rewards = []\n",
        "        self.moving_avg_ep_lengths = []\n",
        "        self.moving_avg_ep_avg_losses = []\n",
        "        self.moving_avg_ep_avg_qs = []\n",
        "\n",
        "        # current episode metric\n",
        "        self.init_episode()\n",
        "\n",
        "        # timing\n",
        "        self.record_time = time.time()\n",
        "\n",
        "    def log_step(self, reward, loss, q):\n",
        "        self.curr_ep_reward += reward\n",
        "        self.curr_ep_length += 1\n",
        "        if loss:\n",
        "            self.curr_ep_loss += loss\n",
        "            self.curr_ep_q += q\n",
        "            self.curr_ep_loss_length += 1\n",
        "\n",
        "    def log_episode(self):\n",
        "        self.ep_rewards.append(self.curr_ep_reward)\n",
        "        self.ep_lengths.append(self.curr_ep_length)\n",
        "        if self.curr_ep_loss_length == 0:\n",
        "            ep_avg_loss = 0\n",
        "            ep_avg_q = 0\n",
        "        else:\n",
        "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
        "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
        "        self.ep_avg_losses.append(ep_avg_loss)\n",
        "        self.ep_avg_qs.append(ep_avg_q)\n",
        "\n",
        "        self.init_episode()\n",
        "\n",
        "    def init_episode(self):\n",
        "        self.curr_ep_reward = 0.0\n",
        "        self.curr_ep_length = 0\n",
        "        self.curr_ep_loss = 0.0\n",
        "        self.curr_ep_q = 0.0\n",
        "        self.curr_ep_loss_length = 0\n",
        "\n",
        "    def record(self, episode, epsilon, step):\n",
        "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
        "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
        "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
        "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
        "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
        "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
        "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
        "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
        "\n",
        "        last_record_time = self.record_time\n",
        "        self.record_time = time.time()\n",
        "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
        "\n",
        "        print(\n",
        "            \"Episode:{:4d}  :: Step:{:5d}  :: Epsilon:{:8.3f}  :: Mean_Reward:{:8.3f}  :: \" \\\n",
        "            \"Mean_Length:{:8.3f}  :: Mean_Loss:{:4.3f}  :: Mean_Q_Value:{:8.3f}  :: \" \\\n",
        "            \"Time_Delta:{:8.3f} \"\n",
        "            .format(episode, step, epsilon, mean_ep_reward, mean_ep_length,\n",
        "                    mean_ep_loss, mean_ep_q, time_since_last_record)\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RObIkqObhLB",
        "outputId": "f8d443e7-63ac-4e88-f140-5221cf41bdb2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7e060401b8b0>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "#path = 'https://github.com/dfreire770/dq-ddq-smb-agent/tree/main/'\n",
        "path ='./'\n",
        "\n",
        "def show_video():\n",
        "    mp4list = glob(path+ '*.mp4')\n",
        "    print(mp4list) # DEBUG\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "                 </video>'''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"Could not find video\")\n",
        "\n",
        "\n",
        "def wrap_env(env):\n",
        "    #env = Monitor(env, path, force=True).  <--- Monitor is deprecated\n",
        "    #env = RecordVideo(env, path)\n",
        "    env = RecordVideo(env, './video', episode_trigger=lambda x: x % 25 == 0)\n",
        "    return env\n",
        "\n",
        "\n",
        "display = Display(visible=0, size=(600, 300))\n",
        "display.start()\n",
        "\n",
        "\n",
        "#env = wrap_env(env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPOi9Bzz_3XE",
        "outputId": "f9d360ad-1379-4509-e10b-54ce0144c582"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using CUDA: False\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode:   0  :: Step:  163  :: Epsilon:   1.000  :: Mean_Reward: 635.000  :: Mean_Length: 163.000  :: Mean_Loss:0.000  :: Mean_Q_Value:   0.000  :: Time_Delta:   1.210 \n",
            "[]\n",
            "Could not find video\n",
            "Episode:  25  :: Step: 4819  :: Epsilon:   0.999  :: Mean_Reward: 630.077  :: Mean_Length: 185.346  :: Mean_Loss:0.000  :: Mean_Q_Value:   0.000  :: Time_Delta:  35.087 \n",
            "[]\n",
            "Could not find video\n",
            "Episode:  50  :: Step:10044  :: Epsilon:   0.997  :: Mean_Reward: 660.451  :: Mean_Length: 196.941  :: Mean_Loss:0.059  :: Mean_Q_Value:   0.036  :: Time_Delta:  39.805 \n",
            "[]\n",
            "Could not find video\n",
            "Episode:  75  :: Step:13948  :: Epsilon:   0.997  :: Mean_Reward: 621.868  :: Mean_Length: 183.526  :: Mean_Loss:0.474  :: Mean_Q_Value:   1.154  :: Time_Delta:  61.012 \n",
            "[]\n",
            "Could not find video\n",
            "Episode: 100  :: Step:20324  :: Epsilon:   0.995  :: Mean_Reward: 662.150  :: Mean_Length: 201.610  :: Mean_Loss:0.490  :: Mean_Q_Value:   1.782  :: Time_Delta: 101.172 \n",
            "[]\n",
            "Could not find video\n",
            "Episode: 125  :: Step:24641  :: Epsilon:   0.994  :: Mean_Reward: 671.230  :: Mean_Length: 198.220  :: Mean_Loss:0.629  :: Mean_Q_Value:   3.382  :: Time_Delta:  68.120 \n",
            "[]\n",
            "Could not find video\n",
            "Episode: 150  :: Step:28912  :: Epsilon:   0.993  :: Mean_Reward: 627.710  :: Mean_Length: 188.680  :: Mean_Loss:0.722  :: Mean_Q_Value:   5.017  :: Time_Delta:  68.213 \n",
            "[]\n",
            "Could not find video\n",
            "Episode: 175  :: Step:36715  :: Epsilon:   0.991  :: Mean_Reward: 671.970  :: Mean_Length: 227.670  :: Mean_Loss:0.531  :: Mean_Q_Value:   6.214  :: Time_Delta: 125.115 \n",
            "[]\n",
            "Could not find video\n",
            "Episode: 200  :: Step:40658  :: Epsilon:   0.990  :: Mean_Reward: 621.270  :: Mean_Length: 203.340  :: Mean_Loss:0.534  :: Mean_Q_Value:   7.559  :: Time_Delta:  63.488 \n",
            "[]\n",
            "Could not find video\n",
            "Episode: 225  :: Step:46079  :: Epsilon:   0.989  :: Mean_Reward: 620.560  :: Mean_Length: 214.380  :: Mean_Loss:0.556  :: Mean_Q_Value:   8.716  :: Time_Delta:  87.469 \n",
            "[]\n",
            "Could not find video\n",
            "Episode: 250  :: Step:51314  :: Epsilon:   0.987  :: Mean_Reward: 641.550  :: Mean_Length: 224.020  :: Mean_Loss:0.595  :: Mean_Q_Value:  10.010  :: Time_Delta:  85.324 \n",
            "[]\n",
            "Could not find video\n",
            "Episode: 275  :: Step:56929  :: Epsilon:   0.986  :: Mean_Reward: 623.150  :: Mean_Length: 202.140  :: Mean_Loss:0.622  :: Mean_Q_Value:  11.214  :: Time_Delta:  93.284 \n",
            "[]\n",
            "Could not find video\n",
            "Episode: 300  :: Step:62081  :: Epsilon:   0.985  :: Mean_Reward: 629.140  :: Mean_Length: 214.230  :: Mean_Loss:0.664  :: Mean_Q_Value:  12.473  :: Time_Delta:  85.538 \n",
            "[]\n",
            "Could not find video\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "if torch.cuda.is_available():\n",
        "    episodes = 301\n",
        "else:\n",
        "   episodes = 26\n",
        "\n",
        "'''\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print(f\"Using CUDA: {use_cuda}\")\n",
        "print()\n",
        "\n",
        "\n",
        "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, use_cuda=use_cuda)\n",
        "\n",
        "logger = MetricLogger()\n",
        "\n",
        "#episodes = 26\n",
        "episodes = 301\n",
        "\n",
        "\n",
        "# record every 25th episode\n",
        "#env = RecordVideo(env, './video', episode_trigger=lambda x: x % 25 == 0) # <---- WHEN TRYING THIS, IT CRASHES\n",
        "\n",
        "# Start the recorder\n",
        "#env.start_video_recorder()  #<----- START VIDEO RECORD\n",
        "\n",
        "for e in range(episodes):\n",
        "    state = env.reset()\n",
        "    #env.render('rgb')\n",
        "\n",
        "    # Play the game!\n",
        "    while True:\n",
        "\n",
        "        # Run agent on the state\n",
        "        action = mario.act(state)\n",
        "\n",
        "        # Agent performs action\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "\n",
        "        #env.render('rgb_array') ### <-- ADDED HERE\n",
        "\n",
        "        # Remember\n",
        "        mario.cache(state, next_state, action, reward, done)\n",
        "\n",
        "        # Learn\n",
        "        q, loss = mario.learn()\n",
        "\n",
        "        # Logging\n",
        "        logger.log_step(reward, loss, q)\n",
        "\n",
        "        # Update state\n",
        "        state = next_state\n",
        "\n",
        "        # Check if end of game\n",
        "        if done or info[\"flag_get\"]:\n",
        "            break\n",
        "\n",
        "    logger.log_episode()\n",
        "\n",
        "    if e % 25 == 0:\n",
        "        logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step)\n",
        "        #env.close_video_recorder() #<----- STOP VIDEO RECORD\n",
        "        #env.close()\n",
        "        show_video()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPn4cWcESUth"
      },
      "outputs": [],
      "source": [
        "show_video()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxwgsRReCGct"
      },
      "outputs": [],
      "source": [
        "state = env.reset()\n",
        "env = wrap_env(env)\n",
        "\n",
        "# Play the game!\n",
        "while True:\n",
        "\n",
        "    # Run agent on the state\n",
        "    action = mario.act(state)\n",
        "\n",
        "    # Agent performs action\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "\n",
        "    # Update state\n",
        "    state = next_state\n",
        "\n",
        "    # Check if end of game\n",
        "    if done or info[\"flag_get\"]:\n",
        "        break\n",
        "\n",
        "#env.close()\n",
        "show_video()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxIvb3V2jI4D"
      },
      "outputs": [],
      "source": [
        "#/content/https:/github.com/dfreire770/dq-ddq-smb-agent/tree/main/rl-video-episode-0.mp4"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNYq5FtZ24Hw22+OECiMPQn",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}